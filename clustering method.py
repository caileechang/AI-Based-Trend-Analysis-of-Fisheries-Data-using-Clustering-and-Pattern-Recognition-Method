# -*- coding: utf-8 -*-
"""Clustering and Pattern Recognition Method

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZHappongeml9f8zK4qFv5-vKIBbtt0W0
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

import pandas as pd

# Load the correct sheet
dataset  = pd.read_excel('https://www.dropbox.com/scl/fi/4cl5zaor1l32ikyudvf2e/Fisheries-Dataset-vessels-fish-landing.xlsx?rlkey=q2ewpeuzj288ewd17rcqxeuie&st=6h4zijb8&dl=1',
                          sheet_name="Fish Landing")
print(dataset.columns)

data=pd.DataFrame(dataset)
data.head()

# Step 1: Check for missing values
print(dataset.isnull().sum())

# Step 2: Remove missing rows and save to new variable
dataset_cleaned = dataset.dropna()

# Step 3: Confirm all missing values are removed
print(dataset_cleaned.isnull().sum())

# Clean and convert 'Fish Landing (Tonnes)' column
dataset['Fish Landing (Tonnes)'] = (
    dataset['Fish Landing (Tonnes)']
    .astype(str)  # Ensure everything is string
    .str.replace(r'[^\d.]', '', regex=True)  # Remove all except digits and decimal
    .replace('', np.nan)  # Replace empty strings with NaN
    .astype(float)  # Convert to float
)

# Then drop rows with missing values
dataset_cleaned = dataset.dropna(subset=['Fish Landing (Tonnes)']).reset_index(drop=True)

print(dataset_cleaned['Fish Landing (Tonnes)'].dtype)
print(dataset_cleaned['Fish Landing (Tonnes)'].head())

import calendar

# Convert full month names to numeric format
dataset_cleaned['Month'] = dataset_cleaned['Month'].apply(
    lambda x: list(calendar.month_name).index(x.strip().title()) if isinstance(x, str) else x
)
print(dataset_cleaned['Month'].unique())  # Should be integers 1‚Äì12
print(dataset_cleaned.dtypes)  # 'Month' should be int64 or float64

"""## Compute the sum of fish landing for monthly and yearly"""

# Convert full month names to integer (1‚Äì12)
import calendar
dataset_cleaned['Month'] = dataset_cleaned['Month'].apply(
    lambda x: list(calendar.month_name).index(x.strip().title()) if isinstance(x, str) else x
)

# Now group and sum
monthly_totals = dataset_cleaned.groupby(['Year', 'Month'])['Fish Landing (Tonnes)'].sum().reset_index()
monthly_totals.rename(columns={'Fish Landing (Tonnes)': 'Total Fish Landing (Tonnes)'}, inplace=True)

# Optional: create MonthYear string
monthly_totals['MonthYear'] = pd.to_datetime(
    monthly_totals['Year'].astype(str) + '-' + monthly_totals['Month'].astype(str).str.zfill(2),
    format='%Y-%m'
)


# View result
monthly_totals.head()

"""## Data Scaling"""

from sklearn.preprocessing import StandardScaler

# Select and scale monthly features
features_monthly = monthly_totals[['Month', 'Total Fish Landing (Tonnes)']]
scaler_monthly = StandardScaler()
scaled_monthly = scaler_monthly.fit_transform(features_monthly)

# Optional: convert to DataFrame
scaled_monthly_df = pd.DataFrame(scaled_monthly, columns=features_monthly.columns)

yearly_totals = dataset_cleaned.groupby('Year')['Fish Landing (Tonnes)'].sum().reset_index()
yearly_totals.rename(columns={'Fish Landing (Tonnes)': 'Total Fish Landing (Tonnes)'}, inplace=True)

features_yearly = yearly_totals[['Year', 'Total Fish Landing (Tonnes)']]
scaler_yearly = StandardScaler()
scaled_yearly = scaler_yearly.fit_transform(features_yearly)

# Optional: convert to DataFrame
scaled_yearly_df = pd.DataFrame(scaled_yearly, columns=features_yearly.columns)

"""## Identify optimal number of clusters"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

def evaluate_kmeans_k(scaled_data, title_prefix):
    silhouette_scores = []
    inertia_scores = []
    k_range = range(2, 11)

    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42)
        labels = kmeans.fit_predict(scaled_data)
        sil_score = silhouette_score(scaled_data, labels)
        inertia = kmeans.inertia_
        silhouette_scores.append(sil_score)
        inertia_scores.append(inertia)
        print(f"K={k}: Silhouette Score={sil_score:.4f}, Inertia={inertia:.2f}")

    # Find best K by silhouette
    best_index = silhouette_scores.index(max(silhouette_scores))
    best_k = k_range[best_index]
    best_sil = silhouette_scores[best_index]
    best_inertia = inertia_scores[best_index]

    print(f"\n‚úÖ {title_prefix} - Best K = {best_k}")
    print(f"üëâ Silhouette Score = {best_sil:.4f}")
    print(f"üëâ Inertia = {best_inertia:.2f}")

    # Plotting
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(k_range, silhouette_scores, marker='o')
    plt.title(f'{title_prefix} - Silhouette Score vs K')
    plt.xlabel('Number of Clusters (K)')
    plt.ylabel('Silhouette Score')
    plt.grid(True)

    plt.subplot(1, 2, 2)
    plt.plot(k_range, inertia_scores, marker='o', color='orange')
    plt.title(f'{title_prefix} - Elbow Method: Inertia vs K')
    plt.xlabel('Number of Clusters (K)')
    plt.ylabel('Inertia (WSS)')
    plt.grid(True)

    plt.tight_layout()
    plt.show()

    return best_k, best_sil, best_inertia

# üîç Run for both
best_k_monthly, best_sil_monthly, best_inertia_monthly = evaluate_kmeans_k(scaled_monthly, "Monthly Fish Landing")
best_k_yearly, best_sil_yearly, best_inertia_yearly = evaluate_kmeans_k(scaled_yearly, "Yearly Fish Landing")

best_k_monthly = 3
best_k_yearly = 5

from sklearn.cluster import KMeans

# Apply to scaled monthly
kmeans_monthly = KMeans(n_clusters=best_k_monthly, random_state=42)
monthly_totals['Cluster'] = kmeans_monthly.fit_predict(scaled_monthly_df)

# Apply to scaled yearly
kmeans_yearly = KMeans(n_clusters=best_k_yearly, random_state=42)
yearly_totals['Cluster'] = kmeans_yearly.fit_predict(scaled_yearly_df)

monthly_totals_sorted = monthly_totals.sort_values(by='MonthYear')

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
sns.lineplot(data=monthly_totals, x='MonthYear', y='Total Fish Landing (Tonnes)', hue='Cluster', marker='o')
plt.title('Monthly Fish Landing Trends by Cluster')
plt.xticks(rotation=45)
plt.grid(True)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 5))
sns.barplot(data=yearly_totals, x='Year', y='Total Fish Landing (Tonnes)', hue='Cluster')
plt.title('Yearly Fish Landing by Cluster')
plt.grid(True)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 5))
sns.barplot(data=yearly_totals, x='Year', y='Total Fish Landing (Tonnes)', hue='Cluster')
plt.yscale('log')
plt.title('Yearly Fish Landing by Cluster (Log Scale)')
plt.grid(True)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 5))
sns.barplot(data=yearly_totals, x='Year', y='Total Fish Landing (Tonnes)', hue='Cluster')
plt.yscale('log')
plt.title('Yearly Fish Landing by Cluster (Log Scale)')
plt.grid(True)
plt.tight_layout()
plt.show()

yearly_state_totals = dataset_cleaned.groupby(['Year', 'State', 'Type of Fish'])['Fish Landing (Tonnes)'].sum().reset_index()

yearly_state_pivot = yearly_state_totals.pivot_table(
    index=['State', 'Year'],
    columns='Type of Fish',
    values='Fish Landing (Tonnes)',
    aggfunc='sum'
).reset_index().fillna(0)

import re

dataset_cleaned['State'] = (
    dataset_cleaned['State']
    .astype(str)
    .str.upper()
    .str.replace(r'\s*/\s*', '/', regex=True)  # Normalize spacing around slashes
    .str.replace(r'\s+', ' ', regex=True)      # Remove multiple spaces
    .str.strip()
)

dataset_cleaned = dataset_cleaned[
    dataset_cleaned['State'].notna() &
    (dataset_cleaned['State'].str.strip() != '') &
    (dataset_cleaned['State'].str.upper() != 'NAN')
]

state_aliases = {
    'JOHOR/JOHORE': 'JOHOR',
    'JOHOR BARAT/WEST JOHORE': 'JOHOR BARAT/WEST JOHOR',
    'JOHOR TIMUR/EAST JOHORE': 'JOHOR TIMUR/EAST JOHOR',
    'MELAKA/MALACCA': 'MELAKA',
    'MELAKA/ MALACCA': 'MELAKA',
    'PULAU PINANG/PENANG': 'PULAU PINANG',
    'PULAU PINANG/ PENANG': 'PULAU PINANG',
    'PULAU PINANG / PENANG': 'PULAU PINANG',
    'PULAU PINANG PENANG': 'PULAU PINANG'
}

# Exclude aggregated entry for Semenanjung Malaysia
dataset_cleaned = dataset_cleaned[dataset_cleaned['State'] != 'MALAYSIA:SEMENANJUNG MALAYSIA(PENINSULAR MALAYSIA)']
# Drop rows with NaN in 'State'
dataset_cleaned = dataset_cleaned.dropna(subset=['State'])
dataset_cleaned['State'] = dataset_cleaned['State'].replace(state_aliases)
print(sorted(dataset_cleaned['State'].unique()))

# Group and pivot for yearly data
yearly_state_totals = dataset_cleaned.groupby(['Year', 'State', 'Type of Fish'])['Fish Landing (Tonnes)'].sum().reset_index()

yearly_state_pivot = yearly_state_totals.pivot_table(
    index=['State', 'Year'],
    columns='Type of Fish',
    values='Fish Landing (Tonnes)',
    aggfunc='sum'
).reset_index().fillna(0)

# Optional: Rename columns for clarity
yearly_state_pivot.columns.name = None
yearly_state_pivot.rename(columns={
    'Freshwater': 'Freshwater (Tonnes)',
    'Marine': 'Marine (Tonnes)'
}, inplace=True)

# Preview
print(yearly_state_pivot.head())

# Group and pivot for monthly data
monthly_state_totals = dataset_cleaned.groupby(['Month', 'State', 'Type of Fish'])['Fish Landing (Tonnes)'].sum().reset_index()

monthly_state_pivot = monthly_state_totals.pivot_table(
    index=['State', 'Month'],
    columns='Type of Fish',
    values='Fish Landing (Tonnes)',
    aggfunc='sum'
).reset_index().fillna(0)

# Optional: Rename columns for clarity
monthly_state_pivot.columns.name = None
monthly_state_pivot.rename(columns={
    'Freshwater': 'Freshwater (Tonnes)',
    'Marine': 'Marine (Tonnes)'
}, inplace=True)

# Preview
print(monthly_state_pivot.head())

# These from previous steps
yearly_state_pivot  # columns: ['State', 'Year', 'Freshwater (Tonnes)', 'Marine (Tonnes)']
monthly_state_pivot # columns: ['State', 'Month', 'Freshwater (Tonnes)', 'Marine (Tonnes)']

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Copy for clustering
yearly_data = yearly_state_pivot.copy()

# Select features to cluster
features_yearly = yearly_data[['Freshwater (Tonnes)', 'Marine (Tonnes)']]

# Scale the features
scaler = StandardScaler()
scaled_yearly = scaler.fit_transform(features_yearly)

# Apply KMeans (you can tune n_clusters based on silhouette score or elbow)
kmeans_yearly = KMeans(n_clusters=3, random_state=42)
yearly_data['Cluster'] = kmeans_yearly.fit_predict(scaled_yearly)

# Preview
print(yearly_data.head())

# Copy for clustering
monthly_data = monthly_state_pivot.copy()

# Select features
features_monthly = monthly_data[['Freshwater (Tonnes)', 'Marine (Tonnes)']]

# Scale
scaled_monthly = scaler.fit_transform(features_monthly)

# Apply KMeans
kmeans_monthly = KMeans(n_clusters=5, random_state=42)
monthly_data['Cluster'] = kmeans_monthly.fit_predict(scaled_monthly)

# Preview
print(monthly_data.head())

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

# STEP 1: Scale features
features = yearly_state_pivot[['Freshwater (Tonnes)', 'Marine (Tonnes)']]
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

# STEP 2: Apply KMeans with K=5
kmeans = KMeans(n_clusters=5, random_state=42)
yearly_state_pivot['Cluster'] = kmeans.fit_predict(scaled_features)

# STEP 3: Group by Year and Cluster for trend plotting
cluster_trends = yearly_state_pivot.groupby(['Year', 'Cluster'])[['Freshwater (Tonnes)', 'Marine (Tonnes)']].mean().reset_index()

# STEP 4: Plot Freshwater Trends
plt.figure(figsize=(12, 6))
sns.lineplot(data=cluster_trends, x='Year', y='Freshwater (Tonnes)', hue='Cluster', marker='o')
plt.title("Yearly Freshwater Fish Landing Trends by Cluster (K=5)")
plt.xlabel("Year")
plt.ylabel("Freshwater Landing (Tonnes)")
plt.grid(True)
plt.tight_layout()
plt.show()

# STEP 5: Plot Marine Trends
plt.figure(figsize=(12, 6))
sns.lineplot(data=cluster_trends, x='Year', y='Marine (Tonnes)', hue='Cluster', marker='o')
plt.title("Yearly Marine Fish Landing Trends by Cluster (K=5)")
plt.xlabel("Year")
plt.ylabel("Marine Landing (Tonnes)")
plt.grid(True)
plt.tight_layout()
plt.show()

from mpl_toolkits.mplot3d import Axes3D

# STEP: 3D Plot - Freshwater vs Marine vs Year
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')

# Plot using cluster_trends data
scatter = ax.scatter(
    cluster_trends['Freshwater (Tonnes)'],
    cluster_trends['Marine (Tonnes)'],
    cluster_trends['Year'],
    c=cluster_trends['Cluster'],
    cmap='viridis',
    s=60
)

ax.set_xlabel('Freshwater (Tonnes)')
ax.set_ylabel('Marine (Tonnes)')
ax.set_zlabel('Year')
ax.set_title('3D Clustering of Fish Landings by Year (K=5)')

plt.tight_layout()
plt.show()

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

inertia = []
K_range = range(2, 10)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_features)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(8, 5))
plt.plot(K_range, inertia, marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia (WCSS)')
plt.grid(True)
plt.show()

from sklearn.metrics import silhouette_score

silhouette_scores = []

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(scaled_features)
    score = silhouette_score(scaled_features, labels)
    silhouette_scores.append(score)

plt.figure(figsize=(8, 5))
plt.plot(K_range, silhouette_scores, marker='o', color='orange')
plt.title('Silhouette Score vs Number of Clusters')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Silhouette Score')
plt.grid(True)
plt.show()

from sklearn.metrics import davies_bouldin_score

db_scores = []

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(scaled_features)
    db_index = davies_bouldin_score(scaled_features, labels)
    db_scores.append(db_index)

plt.figure(figsize=(8, 5))
plt.plot(K_range, db_scores, marker='o', color='purple')
plt.title('Davies-Bouldin Index vs Number of Clusters')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Davies-Bouldin Index (Lower = Better)')
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Sort data properly
yearly_state_pivot_sorted = yearly_state_pivot.sort_values(['State', 'Year'])

# Unique list of states
states = yearly_state_pivot_sorted['State'].unique()

# Loop through and plot
for state in states:
    state_data = yearly_state_pivot_sorted[yearly_state_pivot_sorted['State'] == state]

    plt.figure(figsize=(10, 5))
    sns.lineplot(data=state_data, x='Year', y='Freshwater (Tonnes)', label='Freshwater', marker='o')
    sns.lineplot(data=state_data, x='Year', y='Marine (Tonnes)', label='Marine', marker='o')

    plt.title(f'Yearly Fish Landings in {state}')
    plt.xlabel('Year')
    plt.ylabel('Fish Landing (Tonnes)')
    plt.grid(True)
    plt.tight_layout()
    plt.show()

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# --- STEP 1: Load & prepare df_vessel (you still need this from Excel) ---
df_vessel = pd.read_excel(
    'https://www.dropbox.com/scl/fi/4cl5zaor1l32ikyudvf2e/Fisheries-Dataset-vessels-fish-landing.xlsx?rlkey=q2ewpeuzj288ewd17rcqxeuie&st=6h4zijb8&dl=1',
    sheet_name="Fish Vessels"
)

# Clean df_vessel
for col in ['Inboard Powered', 'Outboard Powered', 'Non-Powered']:
    df_vessel[col] = pd.to_numeric(df_vessel[col], errors='coerce').fillna(0)

df_vessel['Total number of fishing vessels'] = (
    df_vessel['Inboard Powered'] +
    df_vessel['Outboard Powered'] +
    df_vessel['Non-Powered']
)

df_vessel['State'] = df_vessel['State'].str.upper().str.strip()
df_vessel['Year'] = df_vessel['Year'].astype(int)


# --- STEP 2: Use your already cleaned dataset (dataset_cleaned) ---
# Assuming dataset_cleaned is already in memory and looks like:
# ['Year', 'State', 'Month', 'Type of Fish', 'Fish Landing (Tonnes)']

# Group & pivot landing data
yearly_state_totals = dataset_cleaned.groupby(['Year', 'State', 'Type of Fish'])['Fish Landing (Tonnes)'].sum().reset_index()
yearly_state_pivot = yearly_state_totals.pivot_table(
    index=['State', 'Year'],
    columns='Type of Fish',
    values='Fish Landing (Tonnes)',
    aggfunc='sum'
).reset_index().fillna(0)

# Rename for consistency
yearly_state_pivot.rename(columns={
    'Freshwater': 'Freshwater (Tonnes)',
    'Marine': 'Marine (Tonnes)'
}, inplace=True)

# --- STEP 3: Merge with vessel data ---
merged_df = pd.merge(
    yearly_state_pivot,
    df_vessel[['State', 'Year', 'Total number of fishing vessels']],
    on=['State', 'Year'],
    how='inner'
)

# Add combined total
merged_df['Total Fish Landing (Tonnes)'] = (
    merged_df['Freshwater (Tonnes)'] + merged_df['Marine (Tonnes)']
)

# --- STEP 4: Clustering ---
scaler = StandardScaler()
scaled_features = scaler.fit_transform(merged_df[['Total Fish Landing (Tonnes)', 'Total number of fishing vessels']])

kmeans = KMeans(n_clusters=3, random_state=42)
merged_df['Cluster'] = kmeans.fit_predict(scaled_features)

# --- STEP 5: 3D Visualization ---
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')

ax.scatter(
    merged_df['Total number of fishing vessels'],
    merged_df['Total Fish Landing (Tonnes)'],
    merged_df['Year'],
    c=merged_df['Cluster'],
    cmap='viridis',
    s=60
)

ax.set_xlabel('Total Number of Fishing Vessels')
ax.set_ylabel('Total Fish Landing (Tonnes)')
ax.set_zlabel('Year')
ax.set_title('3D K-Means Clustering: Vessels vs Landings vs Year')

plt.tight_layout()
plt.show()

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# --- STEP 1: Group and pivot by Year, Month, State ---
monthly_totals = dataset_cleaned.groupby(['Year', 'Month', 'State', 'Type of Fish'])['Fish Landing (Tonnes)'].sum().reset_index()

monthly_pivot = monthly_totals.pivot_table(
    index=['State', 'Year', 'Month'],
    columns='Type of Fish',
    values='Fish Landing (Tonnes)',
    aggfunc='sum'
).reset_index().fillna(0)

# Rename for clarity
monthly_pivot.rename(columns={
    'Freshwater': 'Freshwater (Tonnes)',
    'Marine': 'Marine (Tonnes)'
}, inplace=True)

# Add total fish landing
monthly_pivot['Total Fish Landing (Tonnes)'] = (
    monthly_pivot['Freshwater (Tonnes)'] + monthly_pivot['Marine (Tonnes)']
)

# --- STEP 2: Prepare monthly vessel data ---
# Assuming df_vessel is already loaded and cleaned

# Repeat each yearly record 12 times to simulate monthly presence
df_vessel_monthly = df_vessel.loc[df_vessel.index.repeat(12)].copy()
df_vessel_monthly['Month'] = list(range(1, 13)) * len(df_vessel)

# Merge with monthly fish landing
merged_monthly_df = pd.merge(
    monthly_pivot,
    df_vessel_monthly[['State', 'Year', 'Month', 'Total number of fishing vessels']],
    on=['State', 'Year', 'Month'],
    how='inner'
)

# --- STEP 3: Scale and Cluster ---
scaler = StandardScaler()
scaled_monthly = scaler.fit_transform(merged_monthly_df[['Total Fish Landing (Tonnes)', 'Total number of fishing vessels']])

kmeans = KMeans(n_clusters=3, random_state=42)
merged_monthly_df['Cluster'] = kmeans.fit_predict(scaled_monthly)

# --- Optional: Visual check ---
print(merged_monthly_df.head())
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# --- STEP 4: 3D Plot ---
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')

# Scatter plot
scatter = ax.scatter(
    merged_monthly_df['Total number of fishing vessels'],
    merged_monthly_df['Total Fish Landing (Tonnes)'],
    merged_monthly_df['Month'],
    c=merged_monthly_df['Cluster'],
    cmap='viridis',
    s=50
)

ax.set_xlabel('Total Number of Fishing Vessels')
ax.set_ylabel('Total Fish Landing (Tonnes)')
ax.set_zlabel('Month')
ax.set_title('3D K-Means Clustering (Monthly): Vessels vs Landings vs Month')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Step 1: Prepare data
# Example dataframe should include:
# 'Total Fish Landing (Tonnes)', 'Total number of fishing vessels'

data = merged_df.groupby(['State', 'Year'])[
    ['Total Fish Landing (Tonnes)', 'Total number of fishing vessels']
].sum().reset_index()

# Step 2: Scale and apply KMeans
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data[['Total Fish Landing (Tonnes)', 'Total number of fishing vessels']])

kmeans = KMeans(n_clusters=3, random_state=42)
data['Cluster'] = kmeans.fit_predict(scaled_data)

# Step 3: Plot
plt.figure(figsize=(14, 11))

sns.scatterplot(
    data=data,
    x='Total number of fishing vessels',
    y='Total Fish Landing (Tonnes)',
    hue='Cluster',
    style='State',  # optional: to differentiate by state
    palette='viridis',
    s=80
)

plt.title('K-Means Clustering: Fish Landings vs Vessels (Yearly)')
plt.xlabel('Total Number of Fishing Vessels')
plt.ylabel('Total Fish Landing (Tonnes)')
plt.grid(True)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Step 1: Prepare data (monthly)
data_monthly = merged_monthly_df.groupby(['State', 'Year', 'Month'])[
    ['Total Fish Landing (Tonnes)', 'Total number of fishing vessels']
].sum().reset_index()

# Step 2: Scale and apply KMeans
scaler = StandardScaler()
scaled_monthly = scaler.fit_transform(data_monthly[['Total Fish Landing (Tonnes)', 'Total number of fishing vessels']])

kmeans = KMeans(n_clusters=3, random_state=42)
data_monthly['Cluster'] = kmeans.fit_predict(scaled_monthly)

# Step 3: Plot
plt.figure(figsize=(14, 11))

sns.scatterplot(
    data=data_monthly,
    x='Total number of fishing vessels',
    y='Total Fish Landing (Tonnes)',
    hue='Cluster',
    style='State',  # Optional: use style to distinguish states
    palette='viridis',
    s=70
)

plt.title('K-Means Clustering: Fish Landings vs Vessels (Monthly)')
plt.xlabel('Total Number of Fishing Vessels')
plt.ylabel('Total Fish Landing (Tonnes)')
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.metrics import silhouette_score

# Reuse the scaled features
inertia = []
silhouette_scores = []
K_range = range(2, 10)  # Try k from 2 to 9

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(scaled_data)

    inertia.append(kmeans.inertia_)  # For Elbow
    silhouette_scores.append(silhouette_score(scaled_data, labels))  # For Silhouette

# Elbow Method
plt.figure(figsize=(8, 4))
plt.plot(K_range, inertia, marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia (WCSS)')
plt.grid(True)
plt.tight_layout()
plt.show()

# Silhouette Score
plt.figure(figsize=(8, 4))
plt.plot(K_range, silhouette_scores, marker='o', color='orange')
plt.title('Silhouette Score vs Number of Clusters')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Silhouette Score')
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.metrics import davies_bouldin_score
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Scale the features (use your actual clustering input)
features = merged_df[['Total Fish Landing (Tonnes)', 'Total number of fishing vessels']]

scaled_features = StandardScaler().fit_transform(features)

# Evaluate DBI for k from 2 to 9
dbi_scores = []
k_values = range(2, 10)

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(scaled_features)
    dbi = davies_bouldin_score(scaled_features, labels)
    dbi_scores.append(dbi)

# Plot DBI scores
plt.figure(figsize=(8, 5))
plt.plot(k_values, dbi_scores, marker='o', color='purple')
plt.title("Davies-Bouldin Index vs Number of Clusters (k)")
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Davies-Bouldin Index (Lower = Better)")
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.metrics import silhouette_score

wcss = []
silhouette_scores = []
K = range(2, 10)

for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(scaled_monthly)

    wcss.append(kmeans.inertia_)  # Elbow method
    silhouette_scores.append(silhouette_score(scaled_monthly, labels))  # Silhouette score

# Plot Elbow Method
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(K, wcss, marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('WCSS (Inertia)')
plt.grid(True)

# Plot Silhouette Score
plt.subplot(1, 2, 2)
plt.plot(K, silhouette_scores, marker='o', color='orange')
plt.title('Silhouette Score vs Number of Clusters')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Silhouette Score')
plt.grid(True)

plt.tight_layout()
plt.show()

from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Scale the features
features = merged_df[['Total Fish Landing (Tonnes)', 'Total number of fishing vessels']]
scaled_features = StandardScaler().fit_transform(features)

# Apply DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(scaled_features)

# Add labels to the DataFrame
merged_df['DBSCAN_Label'] = labels

# Visualize
plt.figure(figsize=(10, 6))
sns.scatterplot(x=scaled_features[:, 0], y=scaled_features[:, 1], hue=labels, palette='tab10')
plt.title('DBSCAN Clustering with Anomaly Detection')
plt.xlabel('Total Fish Landing (Tonnes)')
plt.ylabel('Total number of fishing vessels')
plt.grid(True)
plt.show()

import numpy as np

labels = merged_df['DBSCAN_Label']
n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
n_outliers = list(labels).count(-1)

print(f"Clusters found: {n_clusters}")
print(f"Outliers detected: {n_outliers}")

# All points labeled -1 are anomalies

anomalies = merged_df[merged_df['DBSCAN_Label'] == -1]
print(f"Number of anomalies detected: {len(anomalies)}")
display(anomalies)

from sklearn.neighbors import NearestNeighbors
import numpy as np
import matplotlib.pyplot as plt

# Use the same scaled features you passed to DBSCAN
neighbors = NearestNeighbors(n_neighbors=5)  # min_samples = 5
neighbors_fit = neighbors.fit(scaled_features)
distances, indices = neighbors_fit.kneighbors(scaled_features)

# Take the distance to the 5th nearest neighbor (index 4 since zero-indexed)
distances = np.sort(distances[:, 4])

# Plot the sorted k-distances to find the elbow (suggested eps)
plt.figure(figsize=(10, 5))
plt.plot(distances)
plt.title("k-distance Graph to Choose eps for DBSCAN")
plt.xlabel("Data Points (sorted)")
plt.ylabel("5th Nearest Neighbor Distance")
plt.grid(True)
plt.show()

from sklearn.cluster import DBSCAN

# Apply DBSCAN with optimal eps from KneeLocator
dbscan = DBSCAN(eps=0.45, min_samples=5)
labels = dbscan.fit_predict(scaled_features)

# Assign labels back to your dataset
merged_df['DBSCAN_Label'] = labels

# Filter out anomalies
anomalies = merged_df[merged_df['DBSCAN_Label'] == -1]
print(f"Number of anomalies detected: {len(anomalies)}")
display(anomalies)

from sklearn.cluster import DBSCAN

# Apply DBSCAN with chosen eps from the elbow
dbscan = DBSCAN(eps=0.368, min_samples=5)
merged_df['DBSCAN_Label'] = dbscan.fit_predict(scaled_features)

# Visualize (original values)
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.scatterplot(
    data=merged_df,
    x='Total Fish Landing (Tonnes)',
    y='Total number of fishing vessels',
    hue='DBSCAN_Label',
    palette='tab10'
)
plt.title('DBSCAN Clustering ')
plt.grid(True)
plt.tight_layout()
plt.show()

sns.scatterplot(x=scaled_features[:, 0], y=scaled_features[:, 1], hue=labels, palette='tab10')
plt.title("DBSCAN Clustering")
plt.xlabel("Feature 1 (Scaled)")
plt.ylabel("Feature 2 (Scaled)")
plt.grid(True)
plt.show()

from sklearn.preprocessing import LabelEncoder

#  Step 1: Only encode into a NEW column (don't overwrite 'State')
le = LabelEncoder()
merged_df['State_Code'] = le.fit_transform(merged_df['State'])  # preserve original 'State'

# Step 2: Filter outliers
outliers = merged_df[merged_df['DBSCAN_Label'] == -1]

#  Step 3: Group using 'State' directly (no need to decode anything!)
outlier_summary = outliers.groupby(['State', 'Year']).size().reset_index(name='Outlier Count')

# Step 4: Display
from IPython.display import display
display(outlier_summary.sort_values(by='Outlier Count', ascending=False))

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

scatter = ax.scatter(
    merged_df['Total Fish Landing (Tonnes)'],
    merged_df['Total number of fishing vessels'],
    merged_df['State_Code'],
    c=merged_df['DBSCAN_Label'],
    cmap='tab10',
    s=50
)

ax.set_xlabel('Fish Landing (Tonnes)')
ax.set_ylabel('Number of Vessels')
ax.set_zlabel('State (Encoded)')
ax.set_title('DBSCAN Clustering with Outliers by State (3D)')
plt.colorbar(scatter, label='Cluster Label (-1 = Outlier)')
plt.show()

from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Scale monthly features
features_month = merged_monthly_df[['Total Fish Landing (Tonnes)', 'Total number of fishing vessels']]
scaled_features_month = StandardScaler().fit_transform(features_month)

# Step 2: Apply DBSCAN
dbscan = DBSCAN(eps=0.18, min_samples=5)
merged_monthly_df['DBSCAN_Label'] = dbscan.fit_predict(scaled_features_month)

# Step 3: Get anomalies with month in fo
anomalies_monthly = merged_monthly_df[merged_monthly_df['DBSCAN_Label'] == -1]

# Step 4: Display with Year & Month
print(f"üìå Number of monthly anomalies detected: {len(anomalies_monthly)}")
display(anomalies_monthly[['State', 'Year', 'Month', 'Freshwater (Tonnes)', 'Marine (Tonnes)', 'Total Fish Landing (Tonnes)', 'Total number of fishing vessels']])
plt.figure(figsize=(10, 6))
sns.scatterplot(
    data=merged_monthly_df,
    x='Total Fish Landing (Tonnes)',
    y='Total number of fishing vessels',
    hue='DBSCAN_Label',
    palette='tab10'
)
plt.title("Monthly DBSCAN Clustering with Anomaly Detection")
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.neighbors import NearestNeighbors
import numpy as np

neighbors = NearestNeighbors(n_neighbors=5)
neighbors_fit = neighbors.fit(scaled_features_month)
distances, _ = neighbors_fit.kneighbors(scaled_features_month)
distances = np.sort(distances[:, 4])

plt.figure(figsize=(10, 5))
plt.plot(distances)
plt.title("k-distance plot (monthly) to tune eps")
plt.xlabel("Points (sorted)")
plt.ylabel("5th Nearest Distance")
plt.grid(True)
plt.show()

from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Step 1: Group monthly average freshwater landings
monthly_freshwater = merged_monthly_df.groupby('Month')[['Freshwater (Tonnes)']].mean().reset_index()

# Step 2: Scale features
scaled_fresh = StandardScaler().fit_transform(monthly_freshwater[['Freshwater (Tonnes)']])

# Step 3: DBSCAN
dbscan = DBSCAN(eps=0.18, min_samples=2)
monthly_freshwater['DBSCAN_Label'] = dbscan.fit_predict(scaled_fresh)

# Step 4: Plot
plt.figure(figsize=(10, 6))
sns.scatterplot(
    data=monthly_freshwater,
    x='Month',
    y='Freshwater (Tonnes)',
    hue='DBSCAN_Label',
    palette='tab10',
    s=100
)
plt.title('DBSCAN: Anomaly Detection for Freshwater Fish Landing (Monthly)')
plt.xlabel('Month')
plt.ylabel('Average Freshwater Fish Landing (Tonnes)')
plt.grid(True)
plt.tight_layout()
plt.show()

# Step 5: Anomalies
print("\nüîç Anomalous Months for Freshwater:")
print(monthly_freshwater[monthly_freshwater['DBSCAN_Label'] == -1])

from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import streamlit as st

def hierarchical_clustering(merged_df):
    st.subheader("Hierarchical Clustering (by Year)")

# STEP 1: Scale the yearly features (e.g., Total Fish Landing, Vessels, etc.)
features = merged_df[['Total Fish Landing (Tonnes)', 'Total number of fishing vessels']]
scaled_features = StandardScaler().fit_transform(features)

# STEP 2: Compute linkage matrix (if not done earlier)
linked = linkage(scaled_features, method='ward')

labels =merged_df['Year'].astype(str).tolist()

# STEP 4: Plot dendrogram
plt.figure(figsize=(16, 6))
dendrogram(linked,
           labels=labels,
           orientation='top',
           distance_sort='descending',
           show_leaf_counts=True)
plt.xticks(rotation=90, fontsize=8)
plt.title('Hierarchical Clustering Dendrogram (by Year)')
plt.xlabel('Year')
plt.ylabel('Distance (Euclidean)')
plt.grid(False)
plt.tight_layout()
plt.show()

from scipy.cluster.hierarchy import fcluster

# Assign cluster labels (e.g., 3 clusters)
cluster_labels = fcluster(linked, t=3, criterion='maxclust')
merged_df['Hierarchical_Label'] = cluster_labels

# Scatterplot
import seaborn as sns
import matplotlib.pyplot as plt

sns.scatterplot(
    data=merged_df,
    x='Total Fish Landing (Tonnes)',
    y='Total number of fishing vessels',
    hue='Hierarchical_Label',
    palette='Set2'
)
plt.title('Hierarchical Clustering Results (Scatter Plot)')
plt.grid(True)
plt.show()

cluster_summary =merged_df.groupby('Hierarchical_Label')[['Total Fish Landing (Tonnes)', 'Total number of fishing vessels']].mean()
print(cluster_summary)

sns.lineplot(
    data=merged_df,
    x='Year',
    y='Total Fish Landing (Tonnes)',
    hue='Hierarchical_Label',
    marker='o'
)
plt.title('Fish Landing Trend by Hierarchical Cluster')
plt.grid(True)
plt.show()

sns.clustermap(
   merged_df[['Total Fish Landing (Tonnes)', 'Total number of fishing vessels']],
    method='ward',
    cmap='viridis',
    standard_scale=1
)

from sklearn.metrics import silhouette_score

sil_score = silhouette_score(scaled_features, cluster_labels)
print(f"Silhouette Score: {sil_score:.2f}")
