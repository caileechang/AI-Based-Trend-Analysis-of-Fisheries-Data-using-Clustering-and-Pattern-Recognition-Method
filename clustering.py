# -*- coding: utf-8 -*-
"""Clustering and Pattern Recognition Method

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZHappongeml9f8zK4qFv5-vKIBbtt0W0
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

import pandas as pd

# Load the correct sheet
dataset  = pd.read_excel('https://www.dropbox.com/scl/fi/4cl5zaor1l32ikyudvf2e/Fisheries-Dataset-vessels-fish-landing.xlsx?rlkey=q2ewpeuzj288ewd17rcqxeuie&st=6h4zijb8&dl=1',
                          sheet_name="Fish Landing")
print(dataset.columns)

data=pd.DataFrame(dataset)
data.head()

# Step 1: Check for missing values
print(dataset.isnull().sum())

# Step 2: Remove missing rows and save to new variable
dataset_cleaned = dataset.dropna()

# Step 3: Confirm all missing values are removed
print(dataset_cleaned.isnull().sum())

# Clean and convert 'Fish Landing (Tonnes)' column
dataset['Fish Landing (Tonnes)'] = (
    dataset['Fish Landing (Tonnes)']
    .astype(str)  # Ensure everything is string
    .str.replace(r'[^\d.]', '', regex=True)  # Remove all except digits and decimal
    .replace('', np.nan)  # Replace empty strings with NaN
    .astype(float)  # Convert to float
)

# Then drop rows with missing values
dataset_cleaned = dataset.dropna(subset=['Fish Landing (Tonnes)']).reset_index(drop=True)

print(dataset_cleaned['Fish Landing (Tonnes)'].dtype)
print(dataset_cleaned['Fish Landing (Tonnes)'].head())

import calendar

# Convert full month names to numeric format
dataset_cleaned['Month'] = dataset_cleaned['Month'].apply(
    lambda x: list(calendar.month_name).index(x.strip().title()) if isinstance(x, str) else x
)
print(dataset_cleaned['Month'].unique())  # Should be integers 1‚Äì12
print(dataset_cleaned.dtypes)  # 'Month' should be int64 or float64

"""## Compute the sum of fish landing for monthly and yearly"""

# Convert full month names to integer (1‚Äì12)
import calendar
dataset_cleaned['Month'] = dataset_cleaned['Month'].apply(
    lambda x: list(calendar.month_name).index(x.strip().title()) if isinstance(x, str) else x
)

# Now group and sum
monthly_totals = dataset_cleaned.groupby(['Year', 'Month'])['Fish Landing (Tonnes)'].sum().reset_index()
monthly_totals.rename(columns={'Fish Landing (Tonnes)': 'Total Fish Landing (Tonnes)'}, inplace=True)

# Optional: create MonthYear string
monthly_totals['MonthYear'] = pd.to_datetime(
    monthly_totals['Year'].astype(str) + '-' + monthly_totals['Month'].astype(str).str.zfill(2),
    format='%Y-%m'
)


# View result
monthly_totals.head()

"""## Data Scaling"""

from sklearn.preprocessing import StandardScaler

# Select and scale monthly features
features_monthly = monthly_totals[['Month', 'Total Fish Landing (Tonnes)']]
scaler_monthly = StandardScaler()
scaled_monthly = scaler_monthly.fit_transform(features_monthly)

# Optional: convert to DataFrame
scaled_monthly_df = pd.DataFrame(scaled_monthly, columns=features_monthly.columns)

yearly_totals = dataset_cleaned.groupby('Year')['Fish Landing (Tonnes)'].sum().reset_index()
yearly_totals.rename(columns={'Fish Landing (Tonnes)': 'Total Fish Landing (Tonnes)'}, inplace=True)

features_yearly = yearly_totals[['Year', 'Total Fish Landing (Tonnes)']]
scaler_yearly = StandardScaler()
scaled_yearly = scaler_yearly.fit_transform(features_yearly)

# Optional: convert to DataFrame
scaled_yearly_df = pd.DataFrame(scaled_yearly, columns=features_yearly.columns)

"""## Identify optimal number of clusters"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

def evaluate_kmeans_k(scaled_data, title_prefix):
    silhouette_scores = []
    inertia_scores = []
    k_range = range(2, 11)

    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42)
        labels = kmeans.fit_predict(scaled_data)
        sil_score = silhouette_score(scaled_data, labels)
        inertia = kmeans.inertia_
        silhouette_scores.append(sil_score)
        inertia_scores.append(inertia)
        print(f"K={k}: Silhouette Score={sil_score:.4f}, Inertia={inertia:.2f}")

    # Find best K by silhouette
    best_index = silhouette_scores.index(max(silhouette_scores))
    best_k = k_range[best_index]
    best_sil = silhouette_scores[best_index]
    best_inertia = inertia_scores[best_index]

    print(f"\n‚úÖ {title_prefix} - Best K = {best_k}")
    print(f"üëâ Silhouette Score = {best_sil:.4f}")
    print(f"üëâ Inertia = {best_inertia:.2f}")

    # Plotting
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(k_range, silhouette_scores, marker='o')
    plt.title(f'{title_prefix} - Silhouette Score vs K')
    plt.xlabel('Number of Clusters (K)')
    plt.ylabel('Silhouette Score')
    plt.grid(True)

    plt.subplot(1, 2, 2)
    plt.plot(k_range, inertia_scores, marker='o', color='orange')
    plt.title(f'{title_prefix} - Elbow Method: Inertia vs K')
    plt.xlabel('Number of Clusters (K)')
    plt.ylabel('Inertia (WSS)')
    plt.grid(True)

    plt.tight_layout()
    plt.show()

    return best_k, best_sil, best_inertia

# üîç Run for both
best_k_monthly, best_sil_monthly, best_inertia_monthly = evaluate_kmeans_k(scaled_monthly, "Monthly Fish Landing")
best_k_yearly, best_sil_yearly, best_inertia_yearly = evaluate_kmeans_k(scaled_yearly, "Yearly Fish Landing")

best_k_monthly = 3
best_k_yearly = 5

from sklearn.cluster import KMeans

# Apply to scaled monthly
kmeans_monthly = KMeans(n_clusters=best_k_monthly, random_state=42)
monthly_totals['Cluster'] = kmeans_monthly.fit_predict(scaled_monthly_df)

# Apply to scaled yearly
kmeans_yearly = KMeans(n_clusters=best_k_yearly, random_state=42)
yearly_totals['Cluster'] = kmeans_yearly.fit_predict(scaled_yearly_df)

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(14, 6))
sns.lineplot(data=monthly_totals, x='MonthYear', y='Total Fish Landing (Tonnes)', hue='Cluster', marker='o')
plt.title('Monthly Fish Landing Trends by Cluster')
plt.xticks(rotation=45)
plt.grid(True)
plt.tight_layout()
plt.show()

monthly_totals_sorted = monthly_totals.sort_values(by='MonthYear')

plt.figure(figsize=(10, 5))
sns.barplot(data=yearly_totals, x='Year', y='Total Fish Landing (Tonnes)', hue='Cluster')
plt.title('Yearly Fish Landing by Cluster')
plt.grid(True)
plt.tight_layout()
plt.show()

yearly_state_totals = dataset_cleaned.groupby(['Year', 'State', 'Type of Fish'])['Fish Landing (Tonnes)'].sum().reset_index()

yearly_state_pivot = yearly_state_totals.pivot_table(
    index=['State', 'Year'],
    columns='Type of Fish',
    values='Fish Landing (Tonnes)',
    aggfunc='sum'
).reset_index().fillna(0)

import re

dataset_cleaned['State'] = (
    dataset_cleaned['State']
    .astype(str)
    .str.upper()
    .str.replace(r'\s*/\s*', '/', regex=True)  # Normalize spacing around slashes
    .str.replace(r'\s+', ' ', regex=True)      # Remove multiple spaces
    .str.strip()
)

dataset_cleaned = dataset_cleaned[
    dataset_cleaned['State'].notna() &
    (dataset_cleaned['State'].str.strip() != '') &
    (dataset_cleaned['State'].str.upper() != 'NAN')
]

state_aliases = {
    'JOHOR/JOHORE': 'JOHOR',
    'JOHOR BARAT/WEST JOHORE': 'JOHOR BARAT/WEST JOHOR',
    'JOHOR TIMUR/EAST JOHORE': 'JOHOR TIMUR/EAST JOHOR',
    'MELAKA/MALACCA': 'MELAKA',
    'MELAKA/ MALACCA': 'MELAKA',
    'PULAU PINANG/PENANG': 'PULAU PINANG',
    'PULAU PINANG/ PENANG': 'PULAU PINANG',
    'PULAU PINANG / PENANG': 'PULAU PINANG'
}

dataset_cleaned['State'] = dataset_cleaned['State'].replace(state_aliases)
print(sorted(dataset_cleaned['State'].unique()))

# Group and pivot for yearly data
yearly_state_totals = dataset_cleaned.groupby(['Year', 'State', 'Type of Fish'])['Fish Landing (Tonnes)'].sum().reset_index()

yearly_state_pivot = yearly_state_totals.pivot_table(
    index=['State', 'Year'],
    columns='Type of Fish',
    values='Fish Landing (Tonnes)',
    aggfunc='sum'
).reset_index().fillna(0)

# Optional: Rename columns for clarity
yearly_state_pivot.columns.name = None
yearly_state_pivot.rename(columns={
    'Freshwater': 'Freshwater (Tonnes)',
    'Marine': 'Marine (Tonnes)'
}, inplace=True)

# Preview
print(yearly_state_pivot.head())

# Group and pivot for monthly data
monthly_state_totals = dataset_cleaned.groupby(['Month', 'State', 'Type of Fish'])['Fish Landing (Tonnes)'].sum().reset_index()

monthly_state_pivot = monthly_state_totals.pivot_table(
    index=['State', 'Month'],
    columns='Type of Fish',
    values='Fish Landing (Tonnes)',
    aggfunc='sum'
).reset_index().fillna(0)

# Optional: Rename columns for clarity
monthly_state_pivot.columns.name = None
monthly_state_pivot.rename(columns={
    'Freshwater': 'Freshwater (Tonnes)',
    'Marine': 'Marine (Tonnes)'
}, inplace=True)

# Preview
print(monthly_state_pivot.head())

# These from previous steps
yearly_state_pivot  # columns: ['State', 'Year', 'Freshwater (Tonnes)', 'Marine (Tonnes)']
monthly_state_pivot # columns: ['State', 'Month', 'Freshwater (Tonnes)', 'Marine (Tonnes)']

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Copy for clustering
yearly_data = yearly_state_pivot.copy()

# Select features to cluster
features_yearly = yearly_data[['Freshwater (Tonnes)', 'Marine (Tonnes)']]

# Scale the features
scaler = StandardScaler()
scaled_yearly = scaler.fit_transform(features_yearly)

# Apply KMeans (you can tune n_clusters based on silhouette score or elbow)
kmeans_yearly = KMeans(n_clusters=3, random_state=42)
yearly_data['Cluster'] = kmeans_yearly.fit_predict(scaled_yearly)

# Preview
print(yearly_data.head())

# Copy for clustering
monthly_data = monthly_state_pivot.copy()

# Select features
features_monthly = monthly_data[['Freshwater (Tonnes)', 'Marine (Tonnes)']]

# Scale
scaled_monthly = scaler.fit_transform(features_monthly)

# Apply KMeans
kmeans_monthly = KMeans(n_clusters=5, random_state=42)
monthly_data['Cluster'] = kmeans_monthly.fit_predict(scaled_monthly)

# Preview
print(monthly_data.head())

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

# STEP 1: Scale features
features = yearly_state_pivot[['Freshwater (Tonnes)', 'Marine (Tonnes)']]
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

# STEP 2: Apply KMeans with K=5
kmeans = KMeans(n_clusters=5, random_state=42)
yearly_state_pivot['Cluster'] = kmeans.fit_predict(scaled_features)

# STEP 3: Group by Year and Cluster for trend plotting
cluster_trends = yearly_state_pivot.groupby(['Year', 'Cluster'])[['Freshwater (Tonnes)', 'Marine (Tonnes)']].mean().reset_index()

# STEP 4: Plot Freshwater Trends
plt.figure(figsize=(12, 6))
sns.lineplot(data=cluster_trends, x='Year', y='Freshwater (Tonnes)', hue='Cluster', marker='o')
plt.title("Yearly Freshwater Fish Landing Trends by Cluster (K=5)")
plt.xlabel("Year")
plt.ylabel("Freshwater Landing (Tonnes)")
plt.grid(True)
plt.tight_layout()
plt.show()

# STEP 5: Plot Marine Trends
plt.figure(figsize=(12, 6))
sns.lineplot(data=cluster_trends, x='Year', y='Marine (Tonnes)', hue='Cluster', marker='o')
plt.title("Yearly Marine Fish Landing Trends by Cluster (K=5)")
plt.xlabel("Year")
plt.ylabel("Marine Landing (Tonnes)")
plt.grid(True)
plt.tight_layout()
plt.show()

from mpl_toolkits.mplot3d import Axes3D

# STEP: 3D Plot - Freshwater vs Marine vs Year
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')

# Plot using cluster_trends data
scatter = ax.scatter(
    cluster_trends['Freshwater (Tonnes)'],
    cluster_trends['Marine (Tonnes)'],
    cluster_trends['Year'],
    c=cluster_trends['Cluster'],
    cmap='viridis',
    s=60
)

ax.set_xlabel('Freshwater (Tonnes)')
ax.set_ylabel('Marine (Tonnes)')
ax.set_zlabel('Year')
ax.set_title('3D Clustering of Fish Landings by Year (K=5)')

plt.tight_layout()
plt.show()

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

inertia = []
K_range = range(2, 10)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_features)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(8, 5))
plt.plot(K_range, inertia, marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia (WCSS)')
plt.grid(True)
plt.show()

from sklearn.metrics import silhouette_score

silhouette_scores = []

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(scaled_features)
    score = silhouette_score(scaled_features, labels)
    silhouette_scores.append(score)

plt.figure(figsize=(8, 5))
plt.plot(K_range, silhouette_scores, marker='o', color='orange')
plt.title('Silhouette Score vs Number of Clusters')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Silhouette Score')
plt.grid(True)
plt.show()

from sklearn.metrics import davies_bouldin_score

db_scores = []

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(scaled_features)
    db_index = davies_bouldin_score(scaled_features, labels)
    db_scores.append(db_index)

plt.figure(figsize=(8, 5))
plt.plot(K_range, db_scores, marker='o', color='purple')
plt.title('Davies-Bouldin Index vs Number of Clusters')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Davies-Bouldin Index (Lower = Better)')
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Sort data properly
yearly_state_pivot_sorted = yearly_state_pivot.sort_values(['State', 'Year'])

# Unique list of states
states = yearly_state_pivot_sorted['State'].unique()

# Loop through and plot
for state in states:
    state_data = yearly_state_pivot_sorted[yearly_state_pivot_sorted['State'] == state]

    plt.figure(figsize=(10, 5))
    sns.lineplot(data=state_data, x='Year', y='Freshwater (Tonnes)', label='Freshwater', marker='o')
    sns.lineplot(data=state_data, x='Year', y='Marine (Tonnes)', label='Marine', marker='o')

    plt.title(f'Yearly Fish Landings in {state}')
    plt.xlabel('Year')
    plt.ylabel('Fish Landing (Tonnes)')
    plt.grid(True)
    plt.tight_layout()
    plt.show()

import pandas as pd

# Load the correct sheet
df_vessel = pd.read_excel('https://www.dropbox.com/scl/fi/4cl5zaor1l32ikyudvf2e/Fisheries-Dataset-vessels-fish-landing.xlsx?rlkey=q2ewpeuzj288ewd17rcqxeuie&st=6h4zijb8&dl=1',
                          sheet_name="Fish Vessels")
df_landing = pd.read_excel('https://www.dropbox.com/scl/fi/4cl5zaor1l32ikyudvf2e/Fisheries-Dataset-vessels-fish-landing.xlsx?rlkey=q2ewpeuzj288ewd17rcqxeuie&st=6h4zijb8&dl=1', sheet_name='Fish Landing')

# Fill missing values with 0 and convert to numeric
for col in ['Inboard Powered', 'Outboard Powered', 'Non-Powered']:
    df_vessel[col] = pd.to_numeric(df_vessel[col], errors='coerce').fillna(0)

# Create the new column
df_vessel['Total number of fishing vessels'] = (
    df_vessel['Inboard Powered'] +
    df_vessel['Outboard Powered'] +
    df_vessel['Non-Powered']
)

# Optional: Display the result
print(df_vessel[['State', 'Year', 'Total number of fishing vessels']].head())

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Fill missing values and ensure numeric vessel data
for col in ['Inboard Powered', 'Outboard Powered', 'Non-Powered']:
    df_vessel[col] = pd.to_numeric(df_vessel[col], errors='coerce').fillna(0)

# Create total vessel count column
df_vessel['Total number of fishing vessels'] = (
    df_vessel['Inboard Powered'] +
    df_vessel['Outboard Powered'] +
    df_vessel['Non-Powered']
)

# Standardize 'Year' and 'State' for merge
df_vessel['Year'] = df_vessel['Year'].astype(int)
df_vessel['State'] = df_vessel['State'].str.upper().str.strip()
df_landing['Year'] = df_landing['Year'].astype(int)
df_landing['State'] = df_landing['State'].str.upper().str.strip()

# Ensure 'Fish Landing (Tonnes)' is numeric
df_landing['Fish Landing (Tonnes)'] = pd.to_numeric(df_landing['Fish Landing (Tonnes)'], errors='coerce')

# Optionally drop or fill NaNs if any
df_landing = df_landing.dropna(subset=['Fish Landing (Tonnes)'])


# Aggregate fish landings
landing_summary = df_landing.groupby(['State', 'Year'])['Fish Landing (Tonnes)'].sum().reset_index()
landing_summary.rename(columns={'Fish Landing (Tonnes)': 'Total Fish Landing (Tonnes)'}, inplace=True)

# Merge on Year + State
merged_df = pd.merge(landing_summary, df_vessel, on=['State', 'Year'], how='inner')

# Prepare data for clustering
cluster_data = merged_df.groupby(['State', 'Year'])[
    ['Total Fish Landing (Tonnes)', 'Total number of fishing vessels']
].sum().reset_index()

# Scale and apply KMeans
scaler = StandardScaler()
scaled = scaler.fit_transform(cluster_data[['Total Fish Landing (Tonnes)', 'Total number of fishing vessels']])
kmeans = KMeans(n_clusters=3, random_state=42)
cluster_data['Cluster'] = kmeans.fit_predict(scaled)

# Plot 3D clustering
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')
scatter = ax.scatter(
    cluster_data['Total number of fishing vessels'],
    cluster_data['Total Fish Landing (Tonnes)'],
    cluster_data['Year'],
    c=cluster_data['Cluster'],
    cmap='viridis',
    s=60
)

ax.set_xlabel('Total Number of Fishing Vessels')
ax.set_ylabel('Total Fish Landing (Tonnes)')
ax.set_zlabel('Year')
ax.set_title('3D K-Means Clustering: Vessels vs Landings vs Year')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Step 1: Prepare data
# Example dataframe should include:
# 'Total Fish Landing (Tonnes)', 'Total number of fishing vessels'

data = merged_df.groupby(['State', 'Year'])[
    ['Total Fish Landing (Tonnes)', 'Total number of fishing vessels']
].sum().reset_index()

# Step 2: Scale and apply KMeans
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data[['Total Fish Landing (Tonnes)', 'Total number of fishing vessels']])

kmeans = KMeans(n_clusters=3, random_state=42)
data['Cluster'] = kmeans.fit_predict(scaled_data)

# Step 3: Plot
plt.figure(figsize=(14, 11))

sns.scatterplot(
    data=data,
    x='Total number of fishing vessels',
    y='Total Fish Landing (Tonnes)',
    hue='Cluster',
    style='State',  # optional: to differentiate by state
    palette='viridis',
    s=80
)

plt.title('K-Means Clustering: Fish Landings vs Vessels (Yearly)')
plt.xlabel('Total Number of Fishing Vessels')
plt.ylabel('Total Fish Landing (Tonnes)')
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.metrics import silhouette_score

# Reuse the scaled features
inertia = []
silhouette_scores = []
K_range = range(2, 10)  # Try k from 2 to 9

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(scaled)

    inertia.append(kmeans.inertia_)  # For Elbow
    silhouette_scores.append(silhouette_score(scaled, labels))  # For Silhouette

# Elbow Method
plt.figure(figsize=(8, 4))
plt.plot(K_range, inertia, marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia (WCSS)')
plt.grid(True)
plt.tight_layout()
plt.show()

# Silhouette Score
plt.figure(figsize=(8, 4))
plt.plot(K_range, silhouette_scores, marker='o', color='orange')
plt.title('Silhouette Score vs Number of Clusters')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Silhouette Score')
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.metrics import davies_bouldin_score
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Scale the features (use your actual clustering input)
features = cluster_data[['Total Fish Landing (Tonnes)', 'Total number of fishing vessels']]
scaled_features = StandardScaler().fit_transform(features)

# Evaluate DBI for k from 2 to 9
dbi_scores = []
k_values = range(2, 10)

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(scaled_features)
    dbi = davies_bouldin_score(scaled_features, labels)
    dbi_scores.append(dbi)

# Plot DBI scores
plt.figure(figsize=(8, 5))
plt.plot(k_values, dbi_scores, marker='o', color='purple')
plt.title("Davies-Bouldin Index vs Number of Clusters (k)")
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Davies-Bouldin Index (Lower = Better)")
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Scale the features
features = cluster_data[['Total Fish Landing (Tonnes)', 'Total number of fishing vessels']]
scaled_features = StandardScaler().fit_transform(features)

# Apply DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(scaled_features)

# Add labels to the DataFrame
cluster_data['DBSCAN_Label'] = labels

# Visualize
plt.figure(figsize=(10, 6))
sns.scatterplot(x=scaled_features[:, 0], y=scaled_features[:, 1], hue=labels, palette='tab10')
plt.title('DBSCAN Clustering with Anomaly Detection')
plt.xlabel('Total Fish Landing (Tonnes)')
plt.ylabel('Total number of fishing vessels')
plt.grid(True)
plt.show()

# All points labeled -1 are anomalies

anomalies = cluster_data[cluster_data['DBSCAN_Label'] == -1]
print(f"Number of anomalies detected: {len(anomalies)}")
display(anomalies)

from sklearn.neighbors import NearestNeighbors
import numpy as np
import matplotlib.pyplot as plt

# Use the same scaled features you passed to DBSCAN
neighbors = NearestNeighbors(n_neighbors=5)  # min_samples = 5
neighbors_fit = neighbors.fit(scaled_features)
distances, indices = neighbors_fit.kneighbors(scaled_features)

# Take the distance to the 5th nearest neighbor (index 4 since zero-indexed)
distances = np.sort(distances[:, 4])

# Plot the sorted k-distances to find the elbow (suggested eps)
plt.figure(figsize=(10, 5))
plt.plot(distances)
plt.title("k-distance Graph to Choose eps for DBSCAN")
plt.xlabel("Data Points (sorted)")
plt.ylabel("5th Nearest Neighbor Distance")
plt.grid(True)
plt.show()

dbscan = DBSCAN(eps=0.5, min_samples=5)

!pip install kneed

from kneed import KneeLocator
kneedle = KneeLocator(range(len(distances)), distances, curve="convex", direction="increasing")
print(f"Optimal eps: {distances[kneedle.knee]:.3f}")

from sklearn.cluster import DBSCAN

# Apply DBSCAN with optimal eps from KneeLocator
dbscan = DBSCAN(eps=1.404, min_samples=5)
labels = dbscan.fit_predict(scaled_features)

# Assign labels back to your dataset
cluster_data['DBSCAN_Label'] = labels

# Filter out anomalies
anomalies = cluster_data[cluster_data['DBSCAN_Label'] == -1]
print(f"Number of anomalies detected: {len(anomalies)}")
display(anomalies)

sns.scatterplot(x=scaled_features[:, 0], y=scaled_features[:, 1], hue=labels, palette='tab10')
plt.title("DBSCAN Clustering (eps = 1.404)")
plt.xlabel("Feature 1 (Scaled)")
plt.ylabel("Feature 2 (Scaled)")
plt.grid(True)
plt.show()

from sklearn.preprocessing import LabelEncoder

#  Step 1: Only encode into a NEW column (don't overwrite 'State')
le = LabelEncoder()
cluster_data['State_Code'] = le.fit_transform(cluster_data['State'])  # preserve original 'State'

# Step 2: Filter outliers
outliers = cluster_data[cluster_data['DBSCAN_Label'] == -1]

#  Step 3: Group using 'State' directly (no need to decode anything!)
outlier_summary = outliers.groupby(['State', 'Year']).size().reset_index(name='Outlier Count')

# Step 4: Display
from IPython.display import display
display(outlier_summary.sort_values(by='Outlier Count', ascending=False))

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

scatter = ax.scatter(
    cluster_data['Total Fish Landing (Tonnes)'],
    cluster_data['Total number of fishing vessels'],
    cluster_data['State_Code'],
    c=cluster_data['DBSCAN_Label'],
    cmap='tab10',
    s=50
)

ax.set_xlabel('Fish Landing (Tonnes)')
ax.set_ylabel('Number of Vessels')
ax.set_zlabel('State (Encoded)')
ax.set_title('DBSCAN Clustering with Outliers by State (3D)')
plt.colorbar(scatter, label='Cluster Label (-1 = Outlier)')
plt.show()

from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# STEP 1: Scale the yearly features (e.g., Total Fish Landing, Vessels, etc.)
features = cluster_data[['Total Fish Landing (Tonnes)', 'Total number of fishing vessels']]
scaled_features = StandardScaler().fit_transform(features)

# STEP 2: Compute linkage matrix (if not done earlier)
linked = linkage(scaled_features, method='ward')

labels = cluster_data['Year'].astype(str).tolist()

# STEP 4: Plot dendrogram
plt.figure(figsize=(16, 6))
dendrogram(linked,
           labels=labels,
           orientation='top',
           distance_sort='descending',
           show_leaf_counts=True)
plt.xticks(rotation=90, fontsize=8)
plt.title('Hierarchical Clustering Dendrogram (by Year)')
plt.xlabel('Year')
plt.ylabel('Distance (Euclidean)')
plt.grid(False)
plt.tight_layout()
plt.show()

from scipy.cluster.hierarchy import fcluster

# Assign cluster labels (e.g., 3 clusters)
cluster_labels = fcluster(linked, t=3, criterion='maxclust')
cluster_data['Hierarchical_Label'] = cluster_labels

# Scatterplot
import seaborn as sns
import matplotlib.pyplot as plt

sns.scatterplot(
    data=cluster_data,
    x='Total Fish Landing (Tonnes)',
    y='Total number of fishing vessels',
    hue='Hierarchical_Label',
    palette='Set2'
)
plt.title('Hierarchical Clustering Results (Scatter Plot)')
plt.grid(True)
plt.show()

cluster_summary = cluster_data.groupby('Hierarchical_Label')[['Total Fish Landing (Tonnes)', 'Total number of fishing vessels']].mean()
print(cluster_summary)

sns.lineplot(
    data=cluster_data,
    x='Year',
    y='Total Fish Landing (Tonnes)',
    hue='Hierarchical_Label',
    marker='o'
)
plt.title('Fish Landing Trend by Hierarchical Cluster')
plt.grid(True)
plt.show()

sns.clustermap(
    cluster_data[['Total Fish Landing (Tonnes)', 'Total number of fishing vessels']],
    method='ward',
    cmap='viridis',
    standard_scale=1
)

from sklearn.metrics import silhouette_score

sil_score = silhouette_score(scaled_features, cluster_labels)
print(f"Silhouette Score: {sil_score:.2f}")